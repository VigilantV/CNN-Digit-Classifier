{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN digit classifier (MNIST-as-JPG)\n",
    "\n",
    "Train a small Keras CNN to classify digits **0–9** from grayscale JPGs on disk.\n",
    "\n",
    "Expected layout:\n",
    "\n",
    "```\n",
    "DATA_DIR/0/*.jpg\n",
    "DATA_DIR/1/*.jpg\n",
    "...\n",
    "DATA_DIR/9/*.jpg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick image preview\n",
    "\n",
    "Optional sanity check: display 3 sample images and print their shapes (paths are hard-coded; adjust if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = [\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/8/img_14050.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/4/img_10034.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/3/img_57.jpg\",\n",
    "]\n",
    "\n",
    "sample_imgs = [pltimg.imread(p) for p in sample_paths]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(sample_imgs), figsize=(3 * len(sample_imgs), 3))\n",
    "if len(sample_imgs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (ax, img) in enumerate(zip(axes, sample_imgs), start=1):\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    print(f\"sample_img_{i}.shape:\", img.shape)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline (tf.data)\n",
    "\n",
    "`tf.data` pipeline: decode grayscale JPG → resize to `IMG_SIZE` → rescale to **[0,1]** → one-hot labels.\n",
    "\n",
    "Datasets:\n",
    "- `train_ds`: shuffled + augmented\n",
    "- `val_ds`: no augmentation\n",
    "- `test_ds`: no augmentation, no shuffle (held-out evaluation)\n",
    "\n",
    "Uses `cache()` + `prefetch()` for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "IMG_SIZE = (28, 28)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 1000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "DATA_DIR = \"/kaggle/input/mnistasjpg/trainingSet/trainingSet\"\n",
    "MODEL_PATH = \"model2.keras\"\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "augmentation_layer = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomRotation(10 / 360.0),\n",
    "        tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rescale_layer = tf.keras.layers.Rescaling(1.0 / 255.0)\n",
    "\n",
    "def augment(images, labels):\n",
    "    images = augmentation_layer(images, training=True)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0f3cb",
   "metadata": {},
   "source": [
    "## Train/val/test split\n",
    "\n",
    "Stratified **70/15/15** split per digit folder (`0..9`) under `DATA_DIR`, then shuffle within each split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Stratified 70/15/15 split per digit folder\n",
    "train_paths, train_labels = [], []\n",
    "val_paths, val_labels = [], []\n",
    "test_paths, test_labels = [], []\n",
    "\n",
    "for d in range(10):\n",
    "    cls_paths = sorted(str(p) for p in (Path(DATA_DIR) / str(d)).glob(\"*.jpg\"))\n",
    "    rng = np.random.default_rng(SEED + d)\n",
    "    rng.shuffle(cls_paths)\n",
    "\n",
    "    n = len(cls_paths)\n",
    "    n_train = int(0.70 * n)\n",
    "    n_val = int(0.15 * n)\n",
    "    # remainder goes to test to ensure totals match exactly\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_paths += cls_paths[:n_train]\n",
    "    train_labels += [d] * n_train\n",
    "\n",
    "    val_start = n_train\n",
    "    val_end = n_train + n_val\n",
    "    val_paths += cls_paths[val_start:val_end]\n",
    "    val_labels += [d] * n_val\n",
    "\n",
    "    test_paths += cls_paths[val_end:]\n",
    "    test_labels += [d] * n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f52643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle within each split\n",
    "split_rng = np.random.default_rng(SEED)\n",
    "train_perm = split_rng.permutation(len(train_paths))\n",
    "val_perm = split_rng.permutation(len(val_paths))\n",
    "test_perm = split_rng.permutation(len(test_paths))\n",
    "\n",
    "train_paths = [train_paths[i] for i in train_perm]\n",
    "train_labels = [train_labels[i] for i in train_perm]\n",
    "val_paths = [val_paths[i] for i in val_perm]\n",
    "val_labels = [val_labels[i] for i in val_perm]\n",
    "test_paths = [test_paths[i] for i in test_perm]\n",
    "test_labels = [test_labels[i] for i in test_perm]\n",
    "\n",
    "print(\"Dataset split:\")\n",
    "print(f\"- total: {len(train_paths) + len(val_paths) + len(test_paths)}\")\n",
    "print(f\"- train: {len(train_paths)}\")\n",
    "print(f\"- val:   {len(val_paths)}\")\n",
    "print(f\"- test:  {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b093f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "def load_example(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=1)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = rescale_layer(tf.cast(img, tf.float32))\n",
    "    return img, tf.one_hot(label, depth=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cab948",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    .map(load_example, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .shuffle(SHUFFLE_BUFFER, seed=SEED, reshuffle_each_iteration=True)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "    .map(load_example, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "    .map(load_example, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094f239",
   "metadata": {},
   "source": [
    "## Split sanity check\n",
    "\n",
    "Verify the three splits are **disjoint** and roughly **70/15/15** per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8594fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sanity check (part 1): disjointness\n",
    "train_set, val_set, test_set = set(train_paths), set(val_paths), set(test_paths)\n",
    "\n",
    "overlap_train_val = train_set & val_set\n",
    "overlap_train_test = train_set & test_set\n",
    "overlap_val_test = val_set & test_set\n",
    "\n",
    "print(f\"train: {len(train_paths)}\")\n",
    "print(f\"val:   {len(val_paths)}\")\n",
    "print(f\"test:  {len(test_paths)}\")\n",
    "print(f\"overlap (train/val):  {len(overlap_train_val)}\")\n",
    "print(f\"overlap (train/test): {len(overlap_train_test)}\")\n",
    "print(f\"overlap (val/test):   {len(overlap_val_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567362be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check (part 2): per-class counts + overlap examples\n",
    "train_counts = Counter(int(Path(p).parent.name) for p in train_paths)\n",
    "val_counts = Counter(int(Path(p).parent.name) for p in val_paths)\n",
    "test_counts = Counter(int(Path(p).parent.name) for p in test_paths)\n",
    "\n",
    "print(\"per-class (train/val/test):\")\n",
    "for d in range(10):\n",
    "    print(f\"- {d}: {train_counts[d]}/{val_counts[d]}/{test_counts[d]}\")\n",
    "\n",
    "def _print_overlap_examples(name, overlap_paths, limit=10):\n",
    "    if overlap_paths:\n",
    "        print(f\"examples ({name}):\")\n",
    "        for p in sorted(overlap_paths)[:limit]:\n",
    "            print(f\"- {p}\")\n",
    "\n",
    "_print_overlap_examples(\"train/val\", overlap_train_val)\n",
    "_print_overlap_examples(\"train/test\", overlap_train_test)\n",
    "_print_overlap_examples(\"val/test\", overlap_val_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "A small CNN that learns to map each 28×28 grayscale image to one of the 10 digit classes (0–9).\n",
    "\n",
    "Trained with Adam and categorical cross-entropy; learning rate is reduced automatically if validation loss stops improving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8906b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(MODEL_PATH, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5),\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499e714",
   "metadata": {},
   "source": [
    "## Training curve\n",
    "\n",
    "Train vs validation **accuracy** over epochs; a growing gap usually means **overfitting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs validation accuracy\n",
    "acc = history.history.get(\"accuracy\")\n",
    "val_acc = history.history.get(\"val_accuracy\")\n",
    "\n",
    "if acc is None or val_acc is None:\n",
    "    raise KeyError(\n",
    "        \"Missing accuracy history. Available keys: \" + \", \".join(sorted(history.history.keys()))\n",
    "    )\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(epochs, acc, label=\"train accuracy\")\n",
    "plt.plot(epochs, val_acc, label=\"val accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test evaluation (held-out test set)\n",
    "\n",
    "Evaluate the best saved model on the held-out `test_ds` (no augmentation, no shuffle). Optionally plot a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(MODEL_PATH)\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(test_ds, verbose=1)\n",
    "print(f\"test loss: {test_loss:.4f}\")\n",
    "print(f\"test acc:  {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3127966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "all_true = []\n",
    "all_pred = []\n",
    "for images, labels in test_ds:\n",
    "    y_true = tf.argmax(labels, axis=1)\n",
    "    y_pred = tf.argmax(best_model(images, training=False), axis=1)\n",
    "    all_true.append(y_true)\n",
    "    all_pred.append(y_pred)\n",
    "\n",
    "all_true = tf.concat(all_true, axis=0)\n",
    "all_pred = tf.concat(all_pred, axis=0)\n",
    "\n",
    "cm = tf.math.confusion_matrix(all_true, all_pred, num_classes=NUM_CLASSES).numpy()\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion matrix (test set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(NUM_CLASSES))\n",
    "plt.yticks(range(NUM_CLASSES))\n",
    "\n",
    "# annotate cells (small enough for 10 classes)\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = [\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/2/img_7169.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/6/img_16973.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/7/img_23407.jpg\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(sample_paths), figsize=(3 * len(sample_paths), 3))\n",
    "if len(sample_paths) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, path in zip(axes, sample_paths):\n",
    "    pil_img = tf.keras.utils.load_img(path, color_mode=\"grayscale\")\n",
    "    img_array = tf.keras.utils.img_to_array(pil_img)\n",
    "\n",
    "    input_batch = rescale_layer(tf.expand_dims(img_array, axis=0))\n",
    "    pred_probs = best_model.predict(input_batch, verbose=0)\n",
    "    pred_class = int(tf.argmax(pred_probs, axis=1)[0])\n",
    "\n",
    "    ax.imshow(img_array.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(f\"pred: {pred_class}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1272,
     "sourceId": 2280,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1869.59396,
   "end_time": "2025-07-04T05:09:43.468028",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-04T04:38:33.874068",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
