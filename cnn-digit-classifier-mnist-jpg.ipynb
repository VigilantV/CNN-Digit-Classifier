{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN digit classifier (MNIST-as-JPG)\n",
    "\n",
    "Train a small Keras CNN to classify digits **0–9** from grayscale JPGs on disk.\n",
    "\n",
    "Expected layout:\n",
    "\n",
    "```\n",
    "DATA_DIR/0/*.jpg\n",
    "DATA_DIR/1/*.jpg\n",
    "...\n",
    "DATA_DIR/9/*.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick image preview\n",
    "\n",
    "Optional sanity check: display 3 sample images and print their shapes (paths are hard-coded; adjust if needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paths = [\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/8/img_14050.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/4/img_10034.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/3/img_57.jpg\",\n",
    "]\n",
    "\n",
    "sample_imgs = [pltimg.imread(p) for p in sample_paths]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(sample_imgs), figsize=(3 * len(sample_imgs), 3))\n",
    "if len(sample_imgs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (ax, img) in enumerate(zip(axes, sample_imgs), start=1):\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    print(f\"sample_img_{i}.shape:\", img.shape)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline (tf.data)\n",
    "\n",
    "`tf.data` pipeline: decode grayscale JPG → resize to `IMG_SIZE` → rescale to **[0,1]** → one-hot labels.\n",
    "\n",
    "Datasets:\n",
    "- `train_ds`: shuffled + augmented\n",
    "- `val_ds`: no augmentation\n",
    "\n",
    "Uses `cache()` + `prefetch()` for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "IMG_SIZE = (28, 28)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 1000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "DATA_DIR = \"/kaggle/input/mnistasjpg/trainingSet/trainingSet\"\n",
    "MODEL_PATH = \"model2.keras\"\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "augmentation_layer = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomRotation(10 / 360.0),\n",
    "        tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rescale_layer = tf.keras.layers.Rescaling(1.0 / 255.0)\n",
    "\n",
    "def augment(images, labels):\n",
    "    images = augmentation_layer(images, training=True)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0f3cb",
   "metadata": {},
   "source": [
    "## Train/validation split\n",
    "\n",
    "Create a **stratified 80/20** split per digit folder (`0..9`) under `DATA_DIR`, then shuffle within each split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Stratified 80/20 split per digit folder\n",
    "train_paths, train_labels, val_paths, val_labels = [], [], [], []\n",
    "\n",
    "for d in range(10):\n",
    "    cls_paths = sorted(str(p) for p in (Path(DATA_DIR) / str(d)).glob(\"*.jpg\"))\n",
    "    rng = np.random.default_rng(SEED + d)\n",
    "    rng.shuffle(cls_paths)\n",
    "\n",
    "    n_train = int(round(0.8 * len(cls_paths)))\n",
    "    train_paths += cls_paths[:n_train]\n",
    "    train_labels += [d] * n_train\n",
    "    val_paths += cls_paths[n_train:]\n",
    "    val_labels += [d] * (len(cls_paths) - n_train)\n",
    "\n",
    "# Shuffle within each split\n",
    "train_perm = np.random.default_rng(SEED).permutation(len(train_paths))\n",
    "val_perm = np.random.default_rng(SEED).permutation(len(val_paths))\n",
    "train_paths = [train_paths[i] for i in train_perm]\n",
    "train_labels = [train_labels[i] for i in train_perm]\n",
    "val_paths = [val_paths[i] for i in val_perm]\n",
    "val_labels = [val_labels[i] for i in val_perm]\n",
    "\n",
    "print(\"Dataset split:\")\n",
    "print(f\"- total: {len(train_paths) + len(val_paths)}\")\n",
    "print(f\"- train: {len(train_paths)}\")\n",
    "print(f\"- val:   {len(val_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b093f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "def load_example(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=1)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = rescale_layer(tf.cast(img, tf.float32))\n",
    "    return img, tf.one_hot(label, depth=NUM_CLASSES)\n",
    "\n",
    "train_raw = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "val_raw = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "\n",
    "train_base_ds = (\n",
    "    train_raw\n",
    "    .map(load_example, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "train_ds = (\n",
    "    train_base_ds\n",
    "    .shuffle(SHUFFLE_BUFFER, seed=SEED, reshuffle_each_iteration=True)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    val_raw\n",
    "    .map(load_example, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094f239",
   "metadata": {},
   "source": [
    "## Split sanity check\n",
    "\n",
    "Verify the train/val split is **disjoint** (no overlapping file paths) and print per-class counts (should be roughly **80/20**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8594fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sanity check: disjoint + per-class counts\n",
    "train_set, val_set = set(train_paths), set(val_paths)\n",
    "overlap = train_set & val_set\n",
    "\n",
    "print(f\"train: {len(train_paths)}\")\n",
    "print(f\"val:   {len(val_paths)}\")\n",
    "print(f\"overlap: {len(overlap)}\")\n",
    "\n",
    "train_counts = Counter(int(Path(p).parent.name) for p in train_paths)\n",
    "val_counts = Counter(int(Path(p).parent.name) for p in val_paths)\n",
    "print(\"per-class (train/val):\")\n",
    "for d in range(10):\n",
    "    print(f\"- {d}: {train_counts[d]}/{val_counts[d]}\")\n",
    "\n",
    "if overlap:\n",
    "    print(\"examples:\")\n",
    "    for p in sorted(overlap)[:10]:\n",
    "        print(f\"- {p}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "A small CNN that learns to map each 28×28 grayscale image to one of the 10 digit classes (0–9).\n",
    "\n",
    "Trained with Adam and categorical cross-entropy; learning rate is reduced automatically if validation loss stops improving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8906b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(MODEL_PATH, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5),\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499e714",
   "metadata": {},
   "source": [
    "## Training curve\n",
    "\n",
    "Train vs validation **accuracy** over epochs; a growing gap usually means **overfitting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs validation accuracy\n",
    "acc = history.history.get(\"accuracy\")\n",
    "val_acc = history.history.get(\"val_accuracy\")\n",
    "\n",
    "if acc is None or val_acc is None:\n",
    "    raise KeyError(\n",
    "        \"Missing accuracy history. Available keys: \" + \", \".join(sorted(history.history.keys()))\n",
    "    )\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(epochs, acc, label=\"train accuracy\")\n",
    "plt.plot(epochs, val_acc, label=\"val accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference example\n",
    "\n",
    "Load best model from `MODEL_PATH` and predict on 3 images (apply the same `rescale_layer` as training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = [\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/2/img_7169.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/6/img_16973.jpg\",\n",
    "    \"/kaggle/input/mnistasjpg/trainingSet/trainingSet/7/img_23407.jpg\",\n",
    "]\n",
    "\n",
    "model = load_model(MODEL_PATH, compile=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(test_paths), figsize=(3 * len(test_paths), 3))\n",
    "if len(test_paths) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, path in zip(axes, test_paths):\n",
    "    pil_img = tf.keras.utils.load_img(path, color_mode=\"grayscale\")\n",
    "    img_array = tf.keras.utils.img_to_array(pil_img)\n",
    "\n",
    "    input_batch = rescale_layer(np.expand_dims(img_array, axis=0))\n",
    "    pred_probs = model.predict(input_batch, verbose=0)\n",
    "    pred_class = int(np.argmax(pred_probs, axis=1)[0])\n",
    "\n",
    "    ax.imshow(img_array.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(f\"pred: {pred_class}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1272,
     "sourceId": 2280,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1869.59396,
   "end_time": "2025-07-04T05:09:43.468028",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-04T04:38:33.874068",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
